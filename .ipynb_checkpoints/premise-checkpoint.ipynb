{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b258ab5a-78d7-4a92-88b0-03e3d5763781",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib:matplotlib data path: /home/usi/Prevent-2023-RP2/venv/lib/python3.8/site-packages/matplotlib/mpl-data\n",
      "DEBUG:matplotlib:CONFIGDIR=/home/usi/.config/matplotlib\n",
      "DEBUG:matplotlib:interactive is False\n",
      "DEBUG:matplotlib:platform is linux\n",
      "DEBUG:matplotlib:CACHEDIR=/home/usi/.cache/matplotlib\n",
      "DEBUG:matplotlib.font_manager:Using fontManager instance from /home/usi/.cache/matplotlib/fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "import weka.core.jvm as jvm\n",
    "from weka.core.converters import Loader\n",
    "from weka.classifiers import Classifier\n",
    "from weka.core.dataset import create_instances_from_matrices\n",
    "from weka.core.dataset import Attribute, Instance, Instances\n",
    "import weka.core.converters as converters\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from warnings import simplefilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ec9deb3-b43b-4623-854b-78ad02850172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:weka.core.jvm:Adding bundled jars\n",
      "DEBUG:weka.core.jvm:Classpath=['/home/usi/Prevent-2023-RP2/venv/lib/python3.8/site-packages/javabridge/jars/rhino-1.7R4.jar', '/home/usi/Prevent-2023-RP2/venv/lib/python3.8/site-packages/javabridge/jars/runnablequeue.jar', '/home/usi/Prevent-2023-RP2/venv/lib/python3.8/site-packages/javabridge/jars/cpython.jar', '/home/usi/Prevent-2023-RP2/venv/lib/python3.8/site-packages/weka/lib/weka.jar', '/home/usi/Prevent-2023-RP2/venv/lib/python3.8/site-packages/weka/lib/python-weka-wrapper.jar']\n",
      "DEBUG:weka.core.jvm:MaxHeapSize=default\n",
      "DEBUG:javabridge.jutil:Creating JVM object\n",
      "DEBUG:javabridge.jutil:Signalling caller\n"
     ]
    }
   ],
   "source": [
    "# -- Start JVM\n",
    "\n",
    "jvm.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "555c3713-d66e-49d5-b557-1270091e0810",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -- Init the Silencer\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m simplefilter(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mPerformanceWarning)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# -- Init the Silencer\n",
    "\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f328ddc8-59d8-4560-8698-d8d224c36af2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Input Parameters\n",
    "\n",
    "data_set_codes_general = [\"cpu-stress\", \"mem-leak\", \"pack-loss\", \"pack-delay\", \"pack-corr\"]\n",
    "replicas_num = 3\n",
    "approach = \"p\"\n",
    "\n",
    "node_pairs_number = 10\n",
    "number_of_kpis = 1720\n",
    "classifier = \"weka.classifiers.trees.LMT\"\n",
    "options = [\"-I\", \"-1\", \"-M\", \"15\", \"-W\", \"0.0\"]\n",
    "\n",
    "\n",
    "# -- Pathes\n",
    "\n",
    "# Path to the .csv file (Binary Anomaly Matrix)\n",
    "bam_file_path = \"resources/bams_shuffled/{data_set_code}-{node_pair_index}.csv\"\n",
    "\n",
    "# Path to the .csv file to save the point-level predictions\n",
    "predictions_file_path = \"resources/predictions-{approach}/{data_set_code}.csv\"\n",
    "\n",
    "\n",
    "# --- Input Parameters Processing\n",
    "\n",
    "normal_class_label = str(len(data_set_codes_general) * node_pairs_number)\n",
    "number_of_fault_types = len(data_set_codes_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d3a15d-9b53-445b-82df-69707831f904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def load_csv(csv_file_path, source_data_set_code_list, mode, node_pairs_number, faulty_node_pair_index, number_of_kpis, normal_class_label, number_of_fault_types):\n",
    "    \n",
    "    ## Create attributes\n",
    "    attributes = [Attribute.create_numeric(str(ii)) for ii in range(number_of_kpis)]\n",
    "    attributes.append(Attribute.create_nominal(\"class\", [str(fault_index * 10 + node_pair) for fault_index in range(number_of_fault_types) for node_pair in range(node_pairs_number)] + [normal_class_label]))\n",
    "\n",
    "    ## Create dataset\n",
    "    dataset = Instances.create_instances(\"data-set\", attributes, 0)\n",
    "\n",
    "    # Loop by data set groups\n",
    "    for data_set_code_idx, data_set_code in enumerate(source_data_set_code_list):\n",
    "        \n",
    "        # Loop by the data sets inside each group\n",
    "        for node_pair_index in range(0, node_pairs_number):\n",
    "            \n",
    "            # Skip all syntesized (not original) data sets for building a test dataset\n",
    "            if mode == \"test\" and node_pair_index != faulty_node_pair_index:\n",
    "                continue\n",
    "\n",
    "            with open(csv_file_path.format(data_set_code=data_set_code, node_pair_index=node_pair_index), newline='') as csvfile:\n",
    "                csv_reader = csv.reader(csvfile)\n",
    "                next(csv_reader)\n",
    "                for row in csv_reader:\n",
    "                    \n",
    "                    ## Add rows\n",
    "                    vals = [int(item) for item in row[1 : number_of_kpis + 1]]\n",
    "                    \n",
    "                    if row[number_of_kpis + 1] == \"n\":\n",
    "                        label = normal_class_label\n",
    "                    else:\n",
    "                        label = data_set_code_idx * 10 + node_pair_index\n",
    "                        \n",
    "                    vals.append(label)\n",
    "                    \n",
    "                    # Take the normal part only from the first data set in the data set group\n",
    "                    if mode == \"train\" and node_pair_index > 0 and row[number_of_kpis + 1] == \"n\":\n",
    "                        continue\n",
    "\n",
    "                    inst = Instance.create_instance(vals)\n",
    "                    dataset.add_instance(inst)\n",
    "\n",
    "    dataset.class_is_last()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef20a8-c9a1-4d33-9ca8-1e0d35649e11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for replication in range(replicas_num):\n",
    "\n",
    "    if replication == 0:\n",
    "        faulty_pair_index = 9\n",
    "\n",
    "    if replication == 1:\n",
    "        faulty_pair_index = 1\n",
    "\n",
    "    if replication == 2:\n",
    "        faulty_pair_index = 2\n",
    "\n",
    "    for general_code_idx, general_code in enumerate(data_set_codes_general):\n",
    "        data_set_code = \"{code}-{replica}\".format(code=general_code, replica=replication)\n",
    "    \n",
    "        #### Load data\n",
    "        \n",
    "        # Get the codes of the training data sets \n",
    "        source_data_set_code_list_train = [\"{code}-{replica}\".format(code=code, replica=replication) for code in data_set_codes_general if code != general_code]\n",
    "\n",
    "        # Get the code of the testng data set\n",
    "        source_data_set_code_list_test = [data_set_code]\n",
    "            \n",
    "        # Load the data sets\n",
    "        data_set_train = load_csv(bam_file_path, source_data_set_code_list_train, \"train\", node_pairs_number, faulty_pair_index, number_of_kpis, normal_class_label, number_of_fault_types)\n",
    "        data_set_test = load_csv(bam_file_path, source_data_set_code_list_test, \"test\", node_pairs_number, faulty_pair_index, number_of_kpis, normal_class_label, number_of_fault_types)\n",
    "        \n",
    "        print(\"\\nTest Data Set:\", source_data_set_code_list_test)\n",
    "        print(\"Train Data Sets:\", source_data_set_code_list_train)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "        \n",
    "        #### Build the classifier\n",
    "        cls = Classifier(classname=classifier, options=options)\n",
    "\n",
    "        \n",
    "        #### Train the classifier\n",
    "        cls.build_classifier(data_set_train)\n",
    "    \n",
    "        \n",
    "        #### Classify\n",
    "        classifications = []\n",
    "        for minute, instance in enumerate(data_set_test):\n",
    "\n",
    "            prediction = cls.classify_instance(instance)\n",
    "            prediction_probablities_distribution = list(cls.distribution_for_instance(instance))\n",
    "            prediction_probablities_distribution = [round(item, 4) * 100 for item in prediction_probablities_distribution]\n",
    "            prediction_probablities_distribution_sorted = sorted(prediction_probablities_distribution, reverse=True)\n",
    "\n",
    "            probability_1 = prediction_probablities_distribution_sorted[0]\n",
    "            probability_2 = prediction_probablities_distribution_sorted[1]\n",
    "\n",
    "            if probability_1 == probability_2:\n",
    "                suspecious = -1\n",
    "            else:\n",
    "                suspecious = prediction_probablities_distribution.index(probability_1)\n",
    "\n",
    "            print(\"{minute}: suspecious: {suspecious} ({probability}%)\".format(minute=minute+1, suspecious=suspecious, probability=probability_1))\n",
    "            # print(\"distribution=\" + str(prediction_probablities_distribution))\n",
    "\n",
    "            if suspecious == -1:\n",
    "                classification = 1  # No classification -> Prediction. No localization\n",
    "            else:\n",
    "                if suspecious == int(normal_class_label):\n",
    "                    classification = 0  # Classification as Normal -> No Prediction\n",
    "                else:\n",
    "                    suspecious_pair = suspecious % 10\n",
    "                    if suspecious_pair == faulty_node_pair_index:\n",
    "                        classification = 2  # Classification as Faulty. Faulty node pair detected -> Prediction. Localization\n",
    "                    else:\n",
    "                        classification = 1 # Classification as Faulty. Not-Faulty node pair detected -> Prediction. No localization\n",
    "\n",
    "            classifications.append(classification)\n",
    "        \n",
    "        #### Save the classifications\n",
    "        with open(predictions_file_path.format(data_set_code=data_set_code, approach=approach), 'w') as f:\n",
    "            write = csv.writer(f)\n",
    "            for classification in classifications:\n",
    "                write.writerow([classification])\n",
    "\n",
    "        #### Plot the classifications\n",
    "        fig, ax = plt.subplots(figsize=(15, 3))\n",
    "        ax.grid()\n",
    "        ax.set_title(data_set_code)\n",
    "        ax.set_xlim([0, len(classifications)])\n",
    "        ax.set_ylim([0, 3])\n",
    "\n",
    "        for time_point, classification in enumerate(classifications):\n",
    "            if classification == 0:\n",
    "                color = \"w\"\n",
    "                alpha = 1\n",
    "\n",
    "            if classification == 1:\n",
    "                color = \"tab:grey\"\n",
    "                alpha = 1\n",
    "\n",
    "            if classification == 2:\n",
    "                color = \"tab:orange\"\n",
    "                alpha = 1\n",
    "\n",
    "            ax.broken_barh([(time_point, 1)], (0.5, 0.5), facecolors=(color), alpha=alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernel-prevent-23",
   "language": "python",
   "name": "kernel-prevent-23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
