{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 15:56:35.173950: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# **** **** **** **** | Parameters: Global | **** **** **** ****\n",
    "\n",
    "replicas_num = 3\n",
    "consider_test_data_for_fpr_test = True\n",
    "\n",
    "\n",
    "\n",
    "# **** **** **** **** | Parameters: Data sets | **** **** **** ****\n",
    "\n",
    "# AE train data set\n",
    "train_data_set_code = \"normal_1_14\"\n",
    "test_data_set_code = \"normal_24h\"\n",
    "test_data_set_code_for_fpr_test = \"fpr-validation\"\n",
    "\n",
    "# AE test data set\n",
    "test_data_set_code = \"normal_test_24h\"\n",
    "\n",
    "# AE experimental data sets\n",
    "test_data_set_general_codes = [\"cpu-stress\", \"mem-leak\", \"pack-loss\", \"pack-delay\", \"pack-corr\"]\n",
    "\n",
    "if replicas_num > 1:\n",
    "    test_data_set_codes = [\"{code}-{replica}\".format(code=ii, replica=jj) for ii in test_data_set_general_codes for jj in range(replicas_num)]\n",
    "else:\n",
    "    test_data_set_codes = test_data_set_general_codes\n",
    "    \n",
    "# Set of lists with the experiments schedules - [whole duration, fault injection point, failre point]\n",
    "data_sets_config = {\"normal_test_24h\": [1440, 0, 0],\n",
    "                    \"cpu-stress-0\": [45, 16, 42], \"mem-leak-0\": [60, 16, 51], \"pack-loss-0\": [30, 16, 25], \"pack-delay-0\": [30, 16, 25], \"pack-corr-0\": [30, 16, 24],\n",
    "                    \"cpu-stress-1\": [45, 16, 40], \"mem-leak-1\": [60, 16, 43], \"pack-loss-1\": [30, 16, 22], \"pack-delay-1\": [30, 16, 24], \"pack-corr-1\": [30, 16, 28],\n",
    "                    \"cpu-stress-2\": [45, 16, 42], \"mem-leak-2\": [60, 16, 49], \"pack-loss-2\": [30, 16, 23], \"pack-delay-2\": [30, 16, 23], \"pack-corr-2\": [30, 16, 25]}\n",
    "\n",
    "\n",
    "\n",
    "# **** **** **** **** | Parameters: AE Model Training | **** **** **** ****\n",
    "\n",
    "# Randomness\n",
    "seed = 24\n",
    "\n",
    "# Multiplicator for the number of nodes in the hidden layers of the AutoEncoder model\n",
    "number_nodes_factor = 0.5\n",
    "\n",
    "# Number of standard deviations to detect the anomalies (used both for detection on point and metric level) \n",
    "sigma = 3\n",
    "\n",
    "\n",
    "\n",
    "# **** **** **** **** | Parameters: Anomaly Ranker | **** **** **** ****\n",
    "\n",
    "# Anomaly Ranker server port\n",
    "localizer_server_port = 5006\n",
    "\n",
    "# Anomaly Ranker server address\n",
    "server_address = \"http://localhost:{localizer_server_port}\"\n",
    "\n",
    "\n",
    "# Percentage of the top ranked anomalous KPIs to consider for selection of suspicious nodes\n",
    "rank_selection = 20\n",
    "\n",
    "# Cluster node pairs\n",
    "cluster_node_pairs_string = 'redis-1-1 redis-1-12, redis-1-2 redis-1-13, redis-1-3 redis-1-14, redis-1-4 redis-1-15, redis-1-5 redis-1-16, redis-1-6 redis-1-17, redis-1-7 redis-1-18, redis-1-8 redis-1-19, redis-1-9 redis-1-20, redis-1-10 redis-1-11'\n",
    "\n",
    "# Create a list of the cluster's nodes pairs - cluster_node_pairs\n",
    "cluster_node_pairs = cluster_node_pairs_string.split(\", \")\n",
    "for ii in range(len(cluster_node_pairs)):\n",
    "    cluster_node_pairs[ii] = cluster_node_pairs[ii].split(\" \")\n",
    "\n",
    "\n",
    "\n",
    "# **** **** **** **** | Paths | **** **** **** ****\n",
    "\n",
    "# Path to the preprocessed datasets\n",
    "# data_set_file_path = \"/Users/usi/DataHub/Data-Redis-2023/{data_set_code}_tuned/{data_set_code}.csv\"\n",
    "data_set_file_path = \"resources/datasets/{data_set_code}_tuned/{data_set_code}.csv\"\n",
    "\n",
    "# Path to the .csv file with the initial list of features\n",
    "features_file_path_initial = \"resources/features/features_initial.csv\"\n",
    "\n",
    "# Path to the .csv file with the final list of features\n",
    "features_file_path = \"resources/features/features_final.csv\"\n",
    "\n",
    "# Path to the .txt file to save the anomalies in JSON\n",
    "anomalies_file_path = \"resources/anomalies/{data_set_code}.json\"\n",
    "\n",
    "# Path to the .csv file to save the AE point-level predictions\n",
    "predictions_file_path = \"resources/predictions-{approach}/{data_set_code}.csv\"\n",
    "\n",
    "# Path to the .txt file to save the localisations (output of the Anomaly Ranker)\n",
    "localisations_file_path = \"resources/localisations/{data_set_code}.csv\"\n",
    "\n",
    "# Path to the .txt file to save the node-pair level localisations report\n",
    "localisations_by_pairs = \"resources/localisations-pairs/{data_set_code}.csv\"\n",
    "\n",
    "# Path to the .csv file to save the Binary Anomaly Matrixes for Premise\n",
    "bam_file_path = \"resources/bams/{data_set_code}.csv\"\n",
    "\n",
    "# Path to the .csv file to save the Shuffled Binary Anomaly Matrixes for Premise\n",
    "bam_shuffled_file_path = \"resources/bams_shuffled/{data_set_code}-{cluster_node_pair_number}.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# **** **** **** **** | Functions | **** **** **** ****\n",
    "\n",
    "def get_predictions(loss_, threshold_):\n",
    "    return tf.math.greater(loss_, threshold_),\n",
    "\n",
    "def print_stats(predictions_, labels_):\n",
    "    print(\"Accuracy = {}\".format(accuracy_score(labels_, predictions_)))\n",
    "    print(\"Precision = {}\".format(precision_score(labels_, predictions_)))\n",
    "    print(\"Recall = {}\".format(recall_score(labels_, predictions_)))\n",
    "\n",
    "def plot_samples(data_, minute_of_experiment_, title_):\n",
    "    plt.grid()\n",
    "    plt.plot(np.arange(len(data_[minute_of_experiment_])), data_[minute_of_experiment_][:])\n",
    "    plt.title(title_)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_distribution(loss_, title_, color_=\"skyblue\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(loss_[None,:], bins=50, color=color_)\n",
    "    plt.xlabel(\"Loss (reconstruction error)\")\n",
    "    plt.ylabel(\"Number of points\")\n",
    "    plt.title(title_)\n",
    "    plt.show()\n",
    "\n",
    "def get_threshold(loss_, verbose=False):\n",
    "    threshold_up = np.mean(loss_) + sigma * np.std(loss_)\n",
    "    threshold_down = np.mean(loss_) - sigma * np.std(loss_)\n",
    "    if verbose:\n",
    "        print(\"Mean:\", np.mean(loss_),\"Std Deviation:\", np.std(loss_),  \"Threshold UP: \", threshold_up,  \"Threshold DOWN: \", threshold_down)\n",
    "    \n",
    "    return round(np.mean(loss_), 2), round(np.std(loss_), 2), round(threshold_up, 2), round(threshold_down, 2)\n",
    "\n",
    "\n",
    "# Premise -> Bam Shuffling: kpi => metric\n",
    "def get_metric_from_kpi(kpi_):\n",
    "    kpi_components_list_ = kpi_.split(\"_\")\n",
    "    metric_ = \"_\".join(kpi_components_list_[1:])\n",
    "\n",
    "    return metric_\n",
    "\n",
    "# Premise -> Bam Shuffling: kpi, new_recourse -> new_kpi\n",
    "def change_node_in_kpi(kpi_, new_recourse_):\n",
    "    metric_ = get_metric_from_kpi(kpi_)\n",
    "    new_kpi_ = new_recourse_ + \"_\" + metric_\n",
    "\n",
    "    return new_kpi_\n",
    "\n",
    "\n",
    "\n",
    "# **** **** **** **** | Autoencoder model architecture | **** **** **** ****\n",
    "\n",
    "class AnomalyDetector(Model):\n",
    "    def __init__(self, number_of_columns_, factor_):\n",
    "        super(AnomalyDetector, self).__init__()\n",
    "\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            # layers.Dense(int(number_of_columns_ * factor_), activation=\"tanh\", kernel_regularizer=regularizers.l1(0.001)),\n",
    "            layers.Dense(int(number_of_columns_ * factor_), activation=\"tanh\", kernel_regularizer=regularizers.l1(0.002)),\n",
    "            layers.Dense(int(number_of_columns_ * factor_ * factor_), activation=\"tanh\", kernel_regularizer=regularizers.l1(0.002)),\n",
    "            layers.Dense(int(number_of_columns_ * factor_ * factor_ * factor_), activation=\"tanh\"),\n",
    "            # layers.Dense(int(number_of_columns_ * factor_ * factor_ * factor_ * factor_), activation=\"tanh\")\n",
    "        ])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            # layers.Dense(int(number_of_columns_ * factor_ * factor_ * factor_), activation=\"tanh\"),\n",
    "            layers.Dense(int(number_of_columns_ * factor_ * factor_), activation=\"tanh\"),\n",
    "            layers.Dense(int(number_of_columns_ * factor_), activation=\"tanh\"),\n",
    "            layers.Dense(int(number_of_columns_), activation=\"tanh\")\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### Prepare Training Data Set\n",
    "\n",
    "# Load KPI names\n",
    "df_tmp = pd.read_csv(features_file_path, sep=',', header=None)\n",
    "feature_set = df_tmp[0].values\n",
    "\n",
    "# Load Training Data Set\n",
    "train_df_all = pd.read_csv(data_set_file_path.format(data_set_code=train_data_set_code))\n",
    "\n",
    "# Create a numpy.ndarray of the frame's values\n",
    "train_data_all = train_df_all.values.astype(float)\n",
    "\n",
    "# Create labels\n",
    "train_labels_all = np.asarray([0 for ii in range(len(train_data_all))]).astype(bool)\n",
    "\n",
    "# Create Train/Validation Data/Labels arrays\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data_all, train_labels_all, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Build the scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_data)\n",
    "\n",
    "# Normalize the datasets\n",
    "train_data = scaler.transform(train_data)\n",
    "val_data = scaler.transform(val_data)\n",
    "\n",
    "# Transform the Training/Validation data sets to pandas data frames\n",
    "train_df = pd.DataFrame(train_data, columns=list(train_df_all.columns))\n",
    "val_df = pd.DataFrame(val_data, columns=list(train_df_all.columns))\n",
    "\n",
    "# Print summary of the Training/Validation data sets\n",
    "# print(len(train_data), len(train_labels), len(val_data), len(val_labels))\n",
    "\n",
    "# Plot training/validation samples\n",
    "# plot_samples(train_data, 10, \"Training data sample\")\n",
    "# plot_samples(val_data, 10, \"Validation data sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 15:56:45.108109: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "32/32 [==============================] - 3s 41ms/step - loss: 451.1886 - val_loss: 245.4883\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 241.7828 - val_loss: 239.7572\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 239.0636 - val_loss: 238.4113\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 238.0214 - val_loss: 237.5771\n",
      "Epoch 5/40\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 237.2806 - val_loss: 236.9131\n",
      "Epoch 6/40\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 236.6602 - val_loss: 236.3315\n",
      "Epoch 7/40\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 236.1035 - val_loss: 235.7974\n",
      "Epoch 8/40\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 235.5845 - val_loss: 235.2930\n",
      "Epoch 9/40\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 234.7397 - val_loss: 233.9121\n",
      "Epoch 10/40\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 233.7287 - val_loss: 233.4450\n",
      "Epoch 11/40\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 233.2684 - val_loss: 232.9915\n",
      "Epoch 12/40\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 232.8206 - val_loss: 232.5500\n",
      "Epoch 13/40\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 232.3849 - val_loss: 232.1206\n",
      "Epoch 14/40\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 231.9617 - val_loss: 231.7040\n",
      "Epoch 15/40\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 231.5509 - val_loss: 231.2996\n",
      "Epoch 16/40\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 231.1525 - val_loss: 230.9078\n",
      "Epoch 17/40\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 230.7665 - val_loss: 230.5281\n",
      "Epoch 18/40\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 230.3929 - val_loss: 230.1614\n",
      "Epoch 19/40\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 230.0323 - val_loss: 229.8073\n",
      "Epoch 20/40\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 229.6840 - val_loss: 229.4651\n",
      "Epoch 21/40\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 229.3477 - val_loss: 229.1352\n",
      "Epoch 22/40\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 229.0233 - val_loss: 228.8167\n",
      "Epoch 23/40\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 228.7099 - val_loss: 228.5088\n",
      "Epoch 24/40\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 228.4067 - val_loss: 228.2108\n",
      "Epoch 25/40\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 228.1134 - val_loss: 227.9225\n",
      "Epoch 26/40\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 227.8296 - val_loss: 227.6437\n",
      "Epoch 27/40\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 227.5551 - val_loss: 227.3737\n",
      "Epoch 28/40\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 227.2892 - val_loss: 227.1123\n",
      "Epoch 29/40\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 227.0317 - val_loss: 226.8588\n",
      "Epoch 30/40\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 226.7815 - val_loss: 226.6122\n",
      "Epoch 31/40\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 226.5382 - val_loss: 226.3724\n",
      "Epoch 32/40\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 226.1837"
     ]
    }
   ],
   "source": [
    "#### Train the AutoEncoder Model\n",
    "\n",
    "# Set the randomness seed\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "# Build the model\n",
    "autoencoder = AnomalyDetector(len(train_df_all.columns), number_nodes_factor)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss=tf.keras.losses.KLDivergence())\n",
    "\n",
    "# Train the model\n",
    "history = autoencoder.fit(\n",
    "    train_data,\n",
    "    train_data,\n",
    "    epochs=40,\n",
    "    batch_size=512,\n",
    "    validation_data=(val_data, val_data),\n",
    "    use_multiprocessing=False,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Training progress\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculate the Metric-level Reconstruction Error Thresholds (thresholds_per_metric_up)\n",
    "\n",
    "# Get the reconstructions for each KPI of each point of the Training data (output: list of lists of the reconstructed KPIs)\n",
    "reconstructions = autoencoder.predict(train_data)\n",
    "\n",
    "# Build a reconstructions data frame\n",
    "reconstructions_df = pd.DataFrame(reconstructions, columns=list(train_df_all.columns))\n",
    "\n",
    "# Calculate a Reconstruction Error on Training Data for each metric (output: list of MSE's for each metric)\n",
    "reconstruction_errors_df = np.abs(reconstructions_df - train_df)\n",
    "\n",
    "# Collect the Metric-level Thresholds\n",
    "thresholds_per_metric_up = []\n",
    "thresholds_per_metric_down = []\n",
    "\n",
    "for col_idx, col in enumerate(reconstruction_errors_df.columns):\n",
    "    metric_mean, metric_stdev, metric_threshold_up, metric_threshold_down = get_threshold(reconstruction_errors_df[col])\n",
    "    thresholds_per_metric_up.append(metric_threshold_up)\n",
    "    thresholds_per_metric_down.append(metric_threshold_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculate the Point-level Reconstruction Error Threshold - threshold_up\n",
    "\n",
    "# Calculate a Reconstruction Error on Training Data for each point (output: list of MSE's for each point)\n",
    "loss = tf.keras.losses.mse(reconstructions, train_data)\n",
    "\n",
    "# Plot the distribution of reconstruction errors on the training data (Training loss)\n",
    "plot_loss_distribution(loss, \"Train Loss - Distribution of Reconstruction Errors on Training Dataset\")\n",
    "\n",
    "# Calculate the Point-level Anomaly Threshold (the deviation threshold of the Point's MSE) basing on the Training Loss\n",
    "point_level_mean, point_level_stdev, threshold_up, threshold_down  = get_threshold(loss, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Make AE Reconstructions on Experimental Data with seeded anomalies\n",
    "\n",
    "the_test_data_set_codes = test_data_set_codes.copy()\n",
    "\n",
    "if consider_test_data_for_fpr_test:\n",
    "    the_test_data_set_codes.append(test_data_set_code_for_fpr_test)\n",
    "\n",
    "test_data_sets = []\n",
    "test_reconstructions = []\n",
    "\n",
    "for test_data_set_code in the_test_data_set_codes:\n",
    "    \n",
    "    # Download the data set\n",
    "    test_df = pd.read_csv(data_set_file_path.format(data_set_code=test_data_set_code))\n",
    "\n",
    "    # Create a numpy.ndarray of the frame's values\n",
    "    test_data = test_df.values\n",
    "    test_data = test_data.astype(float)\n",
    "\n",
    "    # Normalize the test data\n",
    "    test_data = scaler.transform(test_data)\n",
    "    \n",
    "    #!! Add the data set to the list of the normalized test data sets\n",
    "    test_data_sets.append(test_data)\n",
    "    \n",
    "    # Make AE Predictions - Get the KPIs'reconstructions of each point (output: list of lists of the reconstructed metrics)\n",
    "    the_reconstructions = autoencoder.predict(test_data)\n",
    "    \n",
    "    #!! Add the reconstructions to the list of reconstructions on the test data sets\n",
    "    test_reconstructions.append(the_reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Detect point-level anomalies (predictions), save, and visualize\n",
    "\n",
    "the_test_data_set_codes = test_data_set_codes.copy()\n",
    "if consider_test_data_for_fpr_test:\n",
    "    the_test_data_set_codes.append(test_data_set_code_for_fpr_test)\n",
    "\n",
    "for test_data_set_idx, test_data_set_code in enumerate(the_test_data_set_codes):\n",
    "\n",
    "    # Calculate a point-level reconstruction errors (output: list of MSE's for each point)\n",
    "    test_point_reconstruction_errors = tf.keras.losses.mse(test_reconstructions[test_data_set_idx], test_data_sets[test_data_set_idx])\n",
    "  \n",
    "    # Classify a point as an anomaly if its reconstruction error is greater than the point-level threshold (output: list of predictions per each point)\n",
    "    test_predictions = get_predictions(test_point_reconstruction_errors, threshold_up)\n",
    "    test_predictions = np.array(test_predictions)[0]\n",
    "\n",
    "    # Save the preditions (point level anomalies)\n",
    "    test_predictions_bin = [int(prediction) for prediction in test_predictions]\n",
    "    with open(predictions_file_path.format(data_set_code=test_data_set_code, approach=\"a\"), 'w') as f:\n",
    "        write = csv.writer(f)\n",
    "        for prediction_bin in test_predictions_bin:\n",
    "            write.writerow([prediction_bin])\n",
    "            \n",
    "    # Vizualize the preditions\n",
    "    \n",
    "    # Exclude the normal data set user for FPR estimation\n",
    "    if test_data_set_code not in test_data_set_codes:\n",
    "        continue\n",
    "\n",
    "    # Cut the recinstruction error value from top for the point-level visualization purposes\n",
    "    point_level_re_for_visualisation = []\n",
    "    for point_re in test_point_reconstruction_errors:\n",
    "        if point_re > 5:\n",
    "            point_re = 5\n",
    "        point_level_re_for_visualisation.append(point_re)\n",
    "\n",
    "    # Visualize the point-level reconstruction errors\n",
    "    plt.figure(figsize = (12, 5))\n",
    "    plt.title(\"Data set: {data_set_code}\".format(data_set_code=test_data_set_code))\n",
    "    plt.xlabel(\"Time (Minutes)\")\n",
    "    plt.ylabel(\"Reconstruction Error (Loss function: MSE)\")\n",
    "    plt.axhline(y = point_level_mean, color = 'g', linestyle = '-')\n",
    "    plt.axhline(y = threshold_up, color = 'y', linestyle = '-')\n",
    "    plt.plot(np.arange(len(point_level_re_for_visualisation)), point_level_re_for_visualisation)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Detect metric-level anomalies and save in JSON for Anomaly Ranker\n",
    "\n",
    "the_test_data_set_codes = test_data_set_codes.copy()\n",
    "\n",
    "if consider_test_data_for_fpr_test:\n",
    "    the_test_data_set_codes.append(test_data_set_code_for_fpr_test)\n",
    "\n",
    "for test_data_set_idx, test_data_set_code in enumerate(the_test_data_set_codes):\n",
    "\n",
    "    # Calculate the reconstruction errors for each KPI's value within each point\n",
    "    test_reconstruction_errors = np.abs(test_reconstructions[test_data_set_idx] - test_data_sets[test_data_set_idx])\n",
    "\n",
    "    # Create a two-dim array of anomalies (sets of anomalous KPIs for each point) in JSON format - anomalous_kpis_json\n",
    "    anomalous_kpis_json = []\n",
    "    for current_timestamp, point__ in enumerate(test_reconstruction_errors):\n",
    "\n",
    "        anomalous_kpis_json_one_point  = []\n",
    "        for kpi_index, kpi_re in enumerate(point__):\n",
    "\n",
    "            kpi_re_above_threshold = kpi_re - thresholds_per_metric_up[kpi_index]\n",
    "            if kpi_re_above_threshold >= 0:\n",
    "                \n",
    "                kpi_components_list = feature_set[kpi_index].split(\"_\")\n",
    "                kpi_node = kpi_components_list[0]\n",
    "                kpi_metric = \"_\".join(kpi_components_list[1:])\n",
    "                \n",
    "                anomalous_kpis_json_one_point.append({\n",
    "                    'timestamp': 1522751098000,\n",
    "                    'resource': {\n",
    "                        'name': kpi_node,\n",
    "                    },\n",
    "                    'metric': {\n",
    "                        'name': kpi_metric,\n",
    "                    },\n",
    "                    'value': kpi_re_above_threshold/thresholds_per_metric_up[kpi_index],\n",
    "                    'kpi_index': kpi_index\n",
    "                })\n",
    "\n",
    "        # Append an anomalies to the JSON data structure\n",
    "        anomalous_kpis_json.append(anomalous_kpis_json_one_point)\n",
    "\n",
    "    # Save anomalies in JSON format\n",
    "    with open(anomalies_file_path.format(data_set_code=test_data_set_code), \"w\") as outfile:\n",
    "        json.dump(anomalous_kpis_json, outfile)\n",
    "        outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### Rank Anomalies and save the Rankings (Raw Localizations)\n",
    "\n",
    "the_test_data_set_codes = test_data_set_codes.copy()\n",
    "\n",
    "if consider_test_data_for_fpr_test:\n",
    "    the_test_data_set_codes.append(test_data_set_code_for_fpr_test)\n",
    "\n",
    "for test_data_set_code in the_test_data_set_codes:\n",
    "\n",
    "    # Load anomalies\n",
    "    with open(anomalies_file_path.format(data_set_code=test_data_set_code)) as json_file:\n",
    "        anomalous_kpis_json = json.load(json_file)\n",
    "\n",
    "    with open(localisations_file_path.format(data_set_code=test_data_set_code), \"w\") as file_out:\n",
    "        localisations_writer = csv.writer(file_out)\n",
    "\n",
    "        for i in range(len(anomalous_kpis_json)):\n",
    "            data = {\"anomalies\": anomalous_kpis_json[i]}\n",
    "\n",
    "            # Rank KPIs and localise the faulty/suspected nodes\n",
    "            response = requests.post(server_address.format(localizer_server_port=localizer_server_port) + '/localize?rank_selection=' + str(rank_selection) + '&data_set_code='  + str(test_data_set_code) + '&minute=' + str(i), json=data, headers={\"Content-Type\": \"application/json\"})\n",
    "            response_content_decoded = json.loads(response.content)\n",
    "\n",
    "            localization_row = [str(i), response_content_decoded[\"localization\"]]\n",
    "\n",
    "            for item in response_content_decoded[\"suspected_list\"]:\n",
    "                localization_row.append(item[0])\n",
    "                localization_row.append(item[1])\n",
    "\n",
    "            # Save raw localizations\n",
    "            localisations_writer.writerow(localization_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create BAMs - for Premise\n",
    "\n",
    "for test_data_set_code in test_data_set_codes:\n",
    "    \n",
    "    #### Data Preparation --------------------------------------------------------------------\n",
    "    \n",
    "    # Download the test dataset\n",
    "    test_df = pd.read_csv(data_set_file_path.format(data_set_code=test_data_set_code))\n",
    "\n",
    "    # Create a numpy.ndarray of the frame's values\n",
    "    test_data = test_df.values\n",
    "    test_data = test_data.astype(float)\n",
    "\n",
    "    # Normalize test data\n",
    "    test_data = scaler.transform(test_data)\n",
    "    \n",
    "    #### Make AE Predictions - Get the KPIs'reconstructions of each point (output: list of lists of the reconstructed metrics)\n",
    "    test_reconstructions = autoencoder.predict(test_data)\n",
    "\n",
    "    #### Create/Save BAM (Binary Anomaly Matrix) for Premise -------------------------------------------------------\n",
    "\n",
    "    # Calculate the Metric-level reconstruction errors\n",
    "    test_data_df = pd.DataFrame(test_data)\n",
    "    reconstructions_df = pd.DataFrame(test_reconstructions)\n",
    "    \n",
    "    # Build a reconstruction errors frame\n",
    "    reconstruction_errors_fd = np.abs(reconstructions_df - test_data_df)\n",
    "\n",
    "    # Convert a reconstruction errors frame to array\n",
    "    reconstruction_errors_array = reconstruction_errors_fd.values\n",
    "    reconstruction_errors_array = reconstruction_errors_array.astype(float)\n",
    "\n",
    "    # Init the Binary Anomaly Matrix with zeros\n",
    "    anomalies_array = np.zeros((len(reconstruction_errors_array), len(train_df.columns)), dtype=int)\n",
    "    \n",
    "    # Fulfil the Binary Anomaly Matrix based on the reconstruction error and the reconstruction error threshold for  each metric\n",
    "    # Loop by time points\n",
    "    for ii in range(len(reconstruction_errors_array)):\n",
    "        # Loop by metrics\n",
    "        for jj in range(len(reconstruction_errors_array[ii])):\n",
    "            metric_threshold_up = thresholds_per_metric_up[jj]\n",
    "\n",
    "            # Compare the metric's reconstruction error with the metric's threshold\n",
    "            if reconstruction_errors_array[ii][jj] > metric_threshold_up:\n",
    "                anomalies_array[ii][jj] = 1\n",
    "            else:\n",
    "                anomalies_array[ii][jj] = 0\n",
    "\n",
    "    # Save BAM\n",
    "    initial_features_list = []\n",
    "    with open(features_file_path_initial, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "        for da in data:\n",
    "            initial_features_list.append(da[0])  \n",
    "    \n",
    "    # Patch\n",
    "    # TODO: To Fix the text string format problem\n",
    "    initial_features_list[0] = \"redis-1-1_system.network.in.bytes\"\n",
    "\n",
    "    cutted_features_list = []\n",
    "    with open(features_file_path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "        for da in data:\n",
    "            cutted_features_list.append(da[0])       \n",
    "    \n",
    "    # Patch\n",
    "    # TODO: To Fix the text string format problem\n",
    "    cutted_features_list[0] = \"redis-1-1_system.network.in.bytes\"\n",
    "\n",
    "    \n",
    "    with open(bam_file_path.format(data_set_code=test_data_set_code), \"w\") as file_out:\n",
    "        csv_writer = csv.writer(file_out)\n",
    "        \n",
    "        csv_row = [\"timestamp\"]\n",
    "        \n",
    "        for cc in initial_features_list:\n",
    "            csv_row.append(cc)\n",
    "        \n",
    "        csv_row.append(\"class\")\n",
    "        csv_writer.writerow(csv_row)\n",
    "\n",
    "        for ii in range(len(anomalies_array)):\n",
    "            \n",
    "            if data_sets_config[test_data_set_code][1] == 0: # normal data\n",
    "                class_name = \"n\"\n",
    "            else:   \n",
    "                if ii + 1 < int(data_sets_config[test_data_set_code][1]):\n",
    "                    class_name = \"n\"\n",
    "                else:\n",
    "                    if ii + 1 < int(data_sets_config[test_data_set_code][2]):\n",
    "                        class_name = test_data_set_code\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "            csv_row = []\n",
    "            csv_row.append(str(ii + 1))\n",
    "            \n",
    "            for feature_from_initial in initial_features_list:\n",
    "                try:\n",
    "                    index_in_cutted_features_list = cutted_features_list.index(feature_from_initial)\n",
    "                    the_value = anomalies_array[ii][index_in_cutted_features_list]\n",
    "                except ValueError:\n",
    "                    the_value = 0\n",
    "                \n",
    "                csv_row.append(str(the_value))\n",
    "\n",
    "            csv_row.append(class_name) \n",
    "            csv_writer.writerow(csv_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Shuffle BAMs (for Premise)\n",
    "\n",
    "# Loop by the anomalies data sets\n",
    "for data_set_idx, data_set_code in enumerate(test_data_set_codes):\n",
    "\n",
    "    replication = int(data_set_code[-1])\n",
    "    \n",
    "    if replication == 0:\n",
    "        faulty_pair_index = 9\n",
    "\n",
    "    if replication == 1:\n",
    "        faulty_pair_index = 1\n",
    "\n",
    "    if replication == 2:\n",
    "        faulty_pair_index = 2\n",
    "\n",
    "        # Patch\n",
    "        if data_set_code == \"pack-delay-2\":\n",
    "            faulty_pair_index = 1\n",
    "\n",
    "    # Determine the fault master/slave node names\n",
    "    faulty_master = cluster_node_pairs[faulty_pair_index][0]\n",
    "    faulty_slave = cluster_node_pairs[faulty_pair_index][1]\n",
    "    \n",
    "    # Source file (BAM)\n",
    "    df_original = pd.read_csv(bam_file_path.format(data_set_code=data_set_code))\n",
    "    \n",
    "    # Original Header\n",
    "    header_original = list(df_original.columns)[1 : -1]\n",
    "    \n",
    "    # Loop by the cluster's node pairs\n",
    "    for jj in range(0, len(cluster_node_pairs)):\n",
    "        \n",
    "        # Target file path\n",
    "        anomalies_shuffled_file_path = bam_shuffled_file_path.format(data_set_code=data_set_code, cluster_node_pair_number=str(jj))\n",
    "        \n",
    "        # If the pair is the faulty one\n",
    "        if jj == faulty_pair_index:\n",
    "            df_original.to_csv(anomalies_shuffled_file_path, index=False)\n",
    "            continue\n",
    "\n",
    "        # The names of the the nodes from the current pair\n",
    "        target_master = cluster_node_pairs[jj][0]\n",
    "        target_slave = cluster_node_pairs[jj][1]\n",
    "        \n",
    "        # Create a new header\n",
    "        header_new = []\n",
    "        for kpi in header_original:\n",
    "            node = kpi.split(\"_\")[0]\n",
    "\n",
    "            if node == target_master:\n",
    "                kpi_new = change_node_in_kpi(kpi, faulty_master)\n",
    "\n",
    "            if node == target_slave:\n",
    "                kpi_new = change_node_in_kpi(kpi, faulty_slave)\n",
    "\n",
    "            if node == faulty_master:\n",
    "                kpi_new = change_node_in_kpi(kpi, target_master)\n",
    "\n",
    "            if node == faulty_slave:\n",
    "                kpi_new = change_node_in_kpi(kpi, target_slave)\n",
    "\n",
    "            header_new.append(kpi_new)\n",
    "        \n",
    "        df_new = df_original\n",
    "        df_new.to_csv(anomalies_shuffled_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernel-Prevent-23",
   "language": "python",
   "name": "kernel-prevent-23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
