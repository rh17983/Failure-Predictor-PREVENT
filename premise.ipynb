{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b258ab5a-78d7-4a92-88b0-03e3d5763781",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find Java JRE compatible with x86_64 architecture\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'javabridge._javabridge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mweka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjvm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjvm\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Loader\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassifiers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Classifier\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/weka/core/jvm.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This program is free software: you can redistribute it and/or modify\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# it under the terms of the GNU General Public License as published by\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# the Free Software Foundation, either version 3 of the License, or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# jvm.py\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2014-2016 Fracpete (pythonwekawrapper at gmail dot com)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjavabridge\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/javabridge/__init__.py:38\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#: List of absolute paths to JAR files that are required for the\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#: Javabridge to work.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m JARS \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_jars_dir, name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jar\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrhino-1.7R4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunnablequeue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpython\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m start_vm, kill_vm, vm, activate_awt, deactivate_awt\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attach, detach, get_env\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# JavaScript\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/javabridge/jutil.py:157\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# Has side-effect of preloading dylibs\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     _find_jvm_mac()\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjavabridge\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_javabridge\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_javabridge\u001b[39;00m\n\u001b[1;32m    158\u001b[0m __dead_event \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mEvent()\n\u001b[1;32m    159\u001b[0m __kill \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'javabridge._javabridge'"
     ]
    }
   ],
   "source": [
    "import weka.core.jvm as jvm\n",
    "from weka.core.converters import Loader\n",
    "from weka.classifiers import Classifier\n",
    "from weka.core.dataset import create_instances_from_matrices\n",
    "from weka.core.dataset import Attribute, Instance, Instances\n",
    "import weka.core.converters as converters\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from warnings import simplefilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9deb3-b43b-4623-854b-78ad02850172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Start JVM\n",
    "\n",
    "jvm.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c3713-d66e-49d5-b557-1270091e0810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Init the Silencer\n",
    "\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f328ddc8-59d8-4560-8698-d8d224c36af2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Input Parameters\n",
    "\n",
    "data_set_codes_general = [\"cpu-stress\", \"mem-leak\", \"pack-loss\", \"pack-delay\", \"pack-corr\"]\n",
    "replicas_num = 3\n",
    "approach = \"p\"\n",
    "\n",
    "node_pairs_number = 10\n",
    "number_of_kpis = 1720\n",
    "classifier = \"weka.classifiers.trees.LMT\"\n",
    "options = [\"-I\", \"-1\", \"-M\", \"15\", \"-W\", \"0.0\"]\n",
    "\n",
    "\n",
    "# -- Pathes\n",
    "\n",
    "# Path to the .csv file (Binary Anomaly Matrix)\n",
    "bam_file_path = \"resources/bams_shuffled/{data_set_code}-{node_pair_index}.csv\"\n",
    "\n",
    "# Path to the .csv file to save the point-level predictions\n",
    "predictions_file_path = \"resources/predictions-{approach}/{data_set_code}.csv\"\n",
    "\n",
    "\n",
    "# --- Input Parameters Processing\n",
    "\n",
    "normal_class_label = str(len(data_set_codes_general) * node_pairs_number)\n",
    "number_of_fault_types = len(data_set_codes_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d3a15d-9b53-445b-82df-69707831f904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def load_csv(csv_file_path, source_data_set_code_list, mode, node_pairs_number, faulty_node_pair_index, number_of_kpis, normal_class_label, number_of_fault_types):\n",
    "    \n",
    "    ## Create attributes\n",
    "    attributes = [Attribute.create_numeric(str(ii)) for ii in range(number_of_kpis)]\n",
    "    attributes.append(Attribute.create_nominal(\"class\", [str(fault_index * 10 + node_pair) for fault_index in range(number_of_fault_types) for node_pair in range(node_pairs_number)] + [normal_class_label]))\n",
    "\n",
    "    ## Create dataset\n",
    "    dataset = Instances.create_instances(\"data-set\", attributes, 0)\n",
    "\n",
    "    # Loop by data set groups\n",
    "    for data_set_code_idx, data_set_code in enumerate(source_data_set_code_list):\n",
    "        \n",
    "        # Loop by the data sets inside each group\n",
    "        for node_pair_index in range(0, node_pairs_number):\n",
    "            \n",
    "            # Skip all syntesized (not original) data sets for building a test dataset\n",
    "            if mode == \"test\" and node_pair_index != faulty_node_pair_index:\n",
    "                continue\n",
    "\n",
    "            with open(csv_file_path.format(data_set_code=data_set_code, node_pair_index=node_pair_index), newline='') as csvfile:\n",
    "                csv_reader = csv.reader(csvfile)\n",
    "                next(csv_reader)\n",
    "                for row in csv_reader:\n",
    "                    \n",
    "                    ## Add rows\n",
    "                    vals = [int(item) for item in row[1 : number_of_kpis + 1]]\n",
    "                    \n",
    "                    if row[number_of_kpis + 1] == \"n\":\n",
    "                        label = normal_class_label\n",
    "                    else:\n",
    "                        label = data_set_code_idx * 10 + node_pair_index\n",
    "                        \n",
    "                    vals.append(label)\n",
    "                    \n",
    "                    # Take the normal part only from the first data set in the data set group\n",
    "                    if mode == \"train\" and node_pair_index > 0 and row[number_of_kpis + 1] == \"n\":\n",
    "                        continue\n",
    "\n",
    "                    inst = Instance.create_instance(vals)\n",
    "                    dataset.add_instance(inst)\n",
    "\n",
    "    dataset.class_is_last()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef20a8-c9a1-4d33-9ca8-1e0d35649e11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for replication in range(replicas_num):\n",
    "\n",
    "    if replication == 0:\n",
    "        faulty_pair_index = 9\n",
    "\n",
    "    if replication == 1:\n",
    "        faulty_pair_index = 1\n",
    "\n",
    "    if replication == 2:\n",
    "        faulty_pair_index = 2\n",
    "\n",
    "    for general_code_idx, general_code in enumerate(data_set_codes_general):\n",
    "        data_set_code = \"{code}-{replica}\".format(code=general_code, replica=replication)\n",
    "    \n",
    "        #### Load data\n",
    "        \n",
    "        # Get the codes of the training data sets \n",
    "        source_data_set_code_list_train = [\"{code}-{replica}\".format(code=code, replica=replication) for code in data_set_codes_general if code != general_code]\n",
    "\n",
    "        # Get the code of the testng data set\n",
    "        source_data_set_code_list_test = [data_set_code]\n",
    "            \n",
    "        # Load the data sets\n",
    "        data_set_train = load_csv(bam_file_path, source_data_set_code_list_train, \"train\", node_pairs_number, faulty_pair_index, number_of_kpis, normal_class_label, number_of_fault_types)\n",
    "        data_set_test = load_csv(bam_file_path, source_data_set_code_list_test, \"test\", node_pairs_number, faulty_pair_index, number_of_kpis, normal_class_label, number_of_fault_types)\n",
    "        \n",
    "        print(\"\\nTest Data Set:\", source_data_set_code_list_test)\n",
    "        print(\"Train Data Sets:\", source_data_set_code_list_train)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "        \n",
    "        #### Build the classifier\n",
    "        cls = Classifier(classname=classifier, options=options)\n",
    "\n",
    "        \n",
    "        #### Train the classifier\n",
    "        cls.build_classifier(data_set_train)\n",
    "    \n",
    "        \n",
    "        #### Classify\n",
    "        classifications = []\n",
    "        for minute, instance in enumerate(data_set_test):\n",
    "\n",
    "            prediction = cls.classify_instance(instance)\n",
    "            prediction_probablities_distribution = list(cls.distribution_for_instance(instance))\n",
    "            prediction_probablities_distribution = [round(item, 4) * 100 for item in prediction_probablities_distribution]\n",
    "            prediction_probablities_distribution_sorted = sorted(prediction_probablities_distribution, reverse=True)\n",
    "\n",
    "            probability_1 = prediction_probablities_distribution_sorted[0]\n",
    "            probability_2 = prediction_probablities_distribution_sorted[1]\n",
    "\n",
    "            if probability_1 == probability_2:\n",
    "                suspecious = -1\n",
    "            else:\n",
    "                suspecious = prediction_probablities_distribution.index(probability_1)\n",
    "\n",
    "            print(\"{minute}: suspecious: {suspecious} ({probability}%)\".format(minute=minute+1, suspecious=suspecious, probability=probability_1))\n",
    "            # print(\"distribution=\" + str(prediction_probablities_distribution))\n",
    "\n",
    "            if suspecious == -1:\n",
    "                classification = 1  # No classification -> Prediction. No localization\n",
    "            else:\n",
    "                if suspecious == int(normal_class_label):\n",
    "                    classification = 0  # Classification as Normal -> No Prediction\n",
    "                else:\n",
    "                    suspecious_pair = suspecious % 10\n",
    "                    if suspecious_pair == faulty_node_pair_index:\n",
    "                        classification = 2  # Classification as Faulty. Faulty node pair detected -> Prediction. Localization\n",
    "                    else:\n",
    "                        classification = 1 # Classification as Faulty. Not-Faulty node pair detected -> Prediction. No localization\n",
    "\n",
    "            classifications.append(classification)\n",
    "        \n",
    "        #### Save the classifications\n",
    "        with open(predictions_file_path.format(data_set_code=data_set_code, approach=approach), 'w') as f:\n",
    "            write = csv.writer(f)\n",
    "            for classification in classifications:\n",
    "                write.writerow([classification])\n",
    "\n",
    "        #### Plot the classifications\n",
    "        fig, ax = plt.subplots(figsize=(15, 3))\n",
    "        ax.grid()\n",
    "        ax.set_title(data_set_code)\n",
    "        ax.set_xlim([0, len(classifications)])\n",
    "        ax.set_ylim([0, 3])\n",
    "\n",
    "        for time_point, classification in enumerate(classifications):\n",
    "            if classification == 0:\n",
    "                color = \"w\"\n",
    "                alpha = 1\n",
    "\n",
    "            if classification == 1:\n",
    "                color = \"tab:grey\"\n",
    "                alpha = 1\n",
    "\n",
    "            if classification == 2:\n",
    "                color = \"tab:orange\"\n",
    "                alpha = 1\n",
    "\n",
    "            ax.broken_barh([(time_point, 1)], (0.5, 0.5), facecolors=(color), alpha=alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernel-Prevent-23",
   "language": "python",
   "name": "kernel-prevent-23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
